## Upgrading Cluster

Upgrade kubernetes from v1.31.4 to ~~v1.32.8~~ v1.32.6

```
$ k version
Client Version: v1.32.8
Kustomize Version: v5.5.0
Server Version: v1.31.4
```

update `kube_version` var in `inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml`

```
cd kubesplay
source venv/bin/activate.fish
```

<details>
<summary>failed command</summary>
```
ansible-playbook -i inventory/mycluster/hosts.yaml -u $USERNAME -b -v upgrade-cluster.yml -e kube_version=1.32.8 -e "serial=1" --ask-become-pass --ask-pass
```

Failed as roles/kubesplay_defaults/vars/main/checksums.yml does not contain 1.32.8 yet in [155c1c1](https://github.com/kubernetes-sigs/kubespray/commit/155c1c153162baa9bc8c929b1e294d7b491ad4a8).

I use 1.32.6 instead.
</details>

```
ansible-playbook -i inventory/mycluster/hosts.yaml -u $USERNAME -b -v upgrade-cluster.yml -e kube_version=1.32.6 -e "serial=1" --ask-become-pass --ask-pass
```

```
TASK [network_plugin/cilium : Cilium | Install] *****************************************************************************************************
fatal: [node1-master]: FAILED! => {"changed": true, "cmd": ["/usr/local/bin/cilium", "install", "--version", "1.17.3", "-f", "/etc/kubernetes/cilium-values.yaml", "-f", "/etc/kubernetes/cilium-extra-values.yaml"], "delta": "0:00:27.264050", "end": "2025-08-31 07:43:35.938426", "msg": "non-zero return code", "rc": 1, "start": "2025-08-31 07:43:08.674376", "stderr": "\nError: Unable to install Cilium: Unable to continue with install: ServiceAccount \"cilium\" in namespace \"kube-system\" exists and cannot be imported into the current release: invalid ownership metadata; label validation error: missing key \"app.kubernetes.io/managed-by\": must be set to \"Helm\"; annotation validation error: missing key \"meta.helm.sh/release-name\": must be set to \"cilium\"; annotation validation error: missing key \"meta.helm.sh/release-namespace\": must be set to \"kube-system\"", "stderr_lines": ["", "Error: Unable to install Cilium: Unable to continue with install: ServiceAccount \"cilium\" in namespace \"kube-system\" exists and cannot be imported into the current release: invalid ownership metadata; label validation error: missing key \"app.kubernetes.io/managed-by\": must be set to \"Helm\"; annotation validation error: missing key \"meta.helm.sh/release-name\": must be set to \"cilium\"; annotation validation error: missing key \"meta.helm.sh/release-namespace\": must be set to \"kube-system\""], "stdout": "â„¹ï¸  Using Cilium version 1.17.3\nâ„¹ï¸  Using cluster name \"default\"\nğŸ”® Auto-detected kube-proxy has been installed", "stdout_lines": ["â„¹ï¸  Using Cilium version 1.17.3", "â„¹ï¸  Using cluster name \"default\"", "ğŸ”® Auto-detected kube-proxy has been installed"]}

```

ãªã«ã“ã‚Œ

ã‚ˆãã‚ã‹ã‚‰ã‚“ã‘ã©ciliumã®varã‚’ä¸Šæ›¸ã

```
cp sample/group_vars/k8s_cluster/k8s-net-cilium.yml mycluster/group_vars/k8s_cluster/k8s-net-cilium.yml 
```
https://github.com/kubernetes-sigs/kubespray/issues/12252

ã“ã‚Œã¯`-e "cilium_remove_old_resources=true"`ã§æ²»ã£ãŸ

ãªã‚“ã‹longhornã®podãŒevictã§ããªãã¦nodeãŒdrainã§ããªã„ã¿ãŸã„ãªã®ã§ã€`--disable-eviction`ã§ãƒªãƒˆãƒ©ã‚¤ã—ã¦ã¿ã‚‹

```
ansible-playbook -i inventory/mycluster/hosts.yaml -u $USERNAME -b -v upgrade-cluster.yml -e kube_version=1.32.6 -e "serial=1" -e "cilium_remove_old_resources=true" -e "drain_fallback_enabled=true" --ask-become-pass --ask-pass
```

CiliumãŒå…¨éƒ¨è½ã¡ã¦å…ˆã«é€²ã¾ãªããªã£ãŸã€‚
ã‚‚ã†ä½•ã‚‚ã‚ã‹ã‚‰ãªã„ã€‚
```
$ k get po -n kube-system     -o wide
NAME                                   READY   STATUS                  RESTARTS         AGE     IP              NODE           NOMINATED NODE   READINESS GATES
cilium-envoy-28phn                     1/1     Running                 0                174m    192.168.1.118   node2-worker   <none>           <none>
cilium-envoy-95l79                     0/1     CrashLoopBackOff        38 (4m6s ago)    174m    192.168.1.137   node1-master   <none>           <none>
cilium-envoy-ldw4v                     1/1     Running                 0                174m    192.168.1.99    nucboxg3-1     <none>           <none>
cilium-gh8bl                           0/1     Init:CrashLoopBackOff   38 (3m35s ago)   174m    192.168.1.137   node1-master   <none>           <none>
cilium-operator-5bdf79db87-b552b       1/1     Running                 0                111m    192.168.1.118   node2-worker   <none>           <none>
cilium-operator-5bdf79db87-fzvnw       1/1     Running                 0                24m     192.168.1.99    nucboxg3-1     <none>           <none>
cilium-q4jdf                           0/1     Init:CrashLoopBackOff   43 (111s ago)    174m    192.168.1.99    nucboxg3-1     <none>           <none>
cilium-qwg4d                           0/1     Init:CrashLoopBackOff   43 (2m45s ago)   174m    192.168.1.118   node2-worker   <none>           <none>
coredns-6fdd44cdfd-89whr               0/1     Pending                 0                99m     <none>          <none>         <none>           <none>
coredns-6fdd44cdfd-hpxjb               0/1     Pending                 0                99m     <none>          <none>         <none>           <none>
dns-autoscaler-56cb45595c-c2js9        0/1     Pending                 0                167m    <none>          <none>         <none>           <none>
external-dns-5f87769b4c-4bqjb          0/1     Pending                 0                167m    <none>          <none>         <none>           <none>
kube-apiserver-node1-master            1/1     Running                 0                3h52m   192.168.1.137   node1-master   <none>           <none>
kube-controller-manager-node1-master   1/1     Running                 5 (24m ago)      3h52m   192.168.1.137   node1-master   <none>           <none>
kube-proxy-6l4c5                       1/1     Running                 0                100m    192.168.1.137   node1-master   <none>           <none>
kube-proxy-gkxsj                       1/1     Running                 0                100m    192.168.1.99    nucboxg3-1     <none>           <none>
kube-proxy-tn28n                       1/1     Running                 0                100m    192.168.1.118   node2-worker   <none>           <none>
kube-scheduler-node1-master            1/1     Running                 4 (24m ago)      3h52m   192.168.1.137   node1-master   <none>           <none>
metrics-server-5dff58bc89-c5zg5        0/1     Pending                 0                99m     <none>          <none>         <none>           <none>
nginx-proxy-node2-worker               1/1     Running                 0                100m    192.168.1.118   node2-worker   <none>           <none>
nginx-proxy-nucboxg3-1                 1/1     Running                 0                73m     192.168.1.99    nucboxg3-1     <none>           <none>
nodelocaldns-dfxdz                     1/1     Running                 0                3h50m   192.168.1.99    nucboxg3-1     <none>           <none>
nodelocaldns-ljs4h                     1/1     Running                 0                3h51m   192.168.1.137   node1-master   <none>           <none>
nodelocaldns-xf8fs                     1/1     Running                 2 (3h50m ago)    3h50m   192.168.1.118   node2-worker   <none>           <none>

```

https://github.com/kubernetes-sigs/kubespray/issues/12276

kube_ownerã‚’rootã«å¤‰ãˆã¦å†å®Ÿè¡Œã™ã‚‹ã¨ã†ã¾ãã„ã£ãŸãŒã€cilium-envoyãŒnode1-masterã ã‘crashLoopbackOffã™ã‚‹

### cilium-envoy

```
external/com_github_google_tcmalloc/tcmalloc/system-alloc.cc:625] MmapAligned() failed - unable to allocate with tag (hint, size, alignment) - is something limiting address placement? 0x17e1c0000000 1073741824 1073741824 @ 0x5567c24ec4 0x5567c21300 0x5567c20c08 0x5567c00210 0x5567c1e900 0x5567c1e6d4 0x5567bf59c8 0x5567b083d4 0x5567b050f0 0x7fae498614
external/com_github_google_tcmalloc/tcmalloc/arena.cc:58] FATAL ERROR: Out of memory trying to allocate internal tcmalloc data (bytes, object-size); is something preventing mmap from succeeding (sandbox, VSS limitations)? 131072 632 @ 0x5567c25234 0x5567c002a0 0x5567c1e900 0x5567c1e6d4 0x5567bf59c8 0x5567b083d4 0x5567b050f0 0x7fae498614```
```

https://github.com/envoyproxy/envoy/issues/23339

ã“ã®Issueã«ã‚ˆã‚‹ã¨ã€envoyã¯ã‚«ãƒ¼ãƒãƒ«ã®è¨­å®šãŒå¯¾å¿œã—ã¦ãªã„ã‹ã‚‰raspberry pi OS on raspberry pi 4ã§èµ·å‹•ã§ããªã„ï¼ˆï¼Ÿï¼‰ã¨æ›¸ã‹ã‚Œã¦ã„ã‚‹æ°—ãŒã—ãŸã€‚

ãã‚‚ãã‚‚[cilium-envoy](https://docs.cilium.io/en/latest/security/network/proxy/envoy/)ã£ã¦ä½•ï¼Ÿã£ã¦èª¿ã¹ãŸãŒã€ã“ã‚Œã‚’disableã™ã‚‹ã¨DaemonSetã§ã¯ãªãcilium Podã®ä¸­ã§åˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã¨ã—ã¦Envoyã‚’èµ·å‹•ã™ã‚‹ã‚‰ã—ã„ã€‚

> When Cilium L7 functionality (Ingress, Gateway API, Network Policies with L7 functionality, L7 Protocol Visibility) is enabled or installed in a Kubernetes cluster, the Cilium agent starts an Envoy proxy as separate process within the Cilium agent pod.
> 
> That Envoy proxy instance becomes responsible for proxying all matching L7 requests on that node. As a result, L7 traffic targeted by policies depends on the availability of the Cilium agent pod.
> 
> Alternatively, itâ€™s possible to deploy the Envoy proxy as independently life-cycled DaemonSet called `cilium-envoy` instead of running it from within the Cilium Agent Pod.

å‰ã¯ã“ã‚“ãªDaemonSetãªã‹ã£ãŸæ°—ãŒã™ã‚‹ã®ã¨ã€OSå†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¯ã‚ã¾ã‚Šã‚„ã‚ŠãŸããªã„ã®ã§cilium Podã®ä¸­ã«å…¥ã‚Œã‚‹ã“ã¨ã«ã—ãŸã€‚

> To enable the dedicated Envoy proxy DaemonSet, install Cilium with the Helm value `envoy.enabled` set to `true.`

[kubesprayã®template](https://github.com/kubernetes-sigs/kubespray/blob/v2.28.1/roles/network_plugin/cilium/templates/values.yaml.j2)
ã‚’è¦‹ã‚‹ã¨envoy.enabledãŒconfigurableã§ãªã•ãã†ã€‚
ciliumä»¥å¤–å‹•ã„ã¦ã„ã‚‹ã®ã§ã€ä»–ã®ansibleã‚’å‹•ã‹ã™ã®ã¯é¿ã‘ãŸã„(30åˆ†ã‹ã‹ã‚‹ã—)

kubesprayã§ã‚‚helmã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ãã†ã ã£ãŸã®ã§ã€valuesã‚’æ›´æ–°ã—ã¦ciliumã®chartã‚’upgradeã™ã‚‹ã“ã¨ã«ã—ãŸ

ã¾ãšdry run
``` 
helm upgrade cilium cilium/cilium                                                                                
    --namespace kube-system \
    --reuse-values \
    --set envoy.enabled=false \
    --version 1.17.3 \
    --dry-run \
    --debug
```

`envoy.enabled`ãŒ`false`ã«ãªã£ã¦ãã†ã ã£ãŸã®ã§ã€

``` 
helm upgrade cilium cilium/cilium                                                                                
    --namespace kube-system \
    --reuse-values \
    --set envoy.enabled=false \
    --version 1.17.3
```

ã“ã‚Œã§cilium-envoyãŒæ¶ˆãˆã¦ã‚ˆã†ã‚„ãå…¨ã¦å‹•ã„ãŸã€‚

kubesprayã§ciliumã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¯æ¯å›ãƒˆãƒ©ãƒ–ãƒ«ã«ãªã‚‹ã®ã§ã€ã‚„ã‚ã‚ˆã†ã‹ãªâ€¦
